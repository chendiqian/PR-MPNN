program: main.py
method: bayes
entity: mls-stuttgart
project: sweep-k20-e1
name: sweep-k20-e1
run_cap: 30

metric:
  goal: minimize
  name: val_loss

parameters:
  # read data
  dataset:
    value: zinc
  log_path:
    value: './logs'
  data_path:
    value: './datasets'

  # debugger mode
  num_runs:
    value: 1
  use_wandb:
    value: True
  debug:
    value: False

  # sample configs, shared by random and learnable sampling methods
  sample_configs.sample_policy:
    value: global_topk_directed
  sample_configs.in_place:
    value: False
  sample_configs.sample_k:
    value: 20
  sample_configs.ensemble:
    value: 1
  sample_configs.include_original_graph:
    value: True

  # sampler and its hyperparams
  imle_configs.sampler:
    value: simple
  imle_configs.logits_activation:
    value: None
  imle_configs.num_train_ensemble:
    value: 1
  imle_configs.num_val_ensemble:
    value: 1

  # weighting the edges
  imle_configs.weight_edges:
    values: 
      - marginals
      - logits
    distribution: categorical
  imle_configs.marginals_mask:
    values: 
      - True
      - False
    distribution: categorical

  # upstream model
  imle_configs.model:
    value: "transformer"
  imle_configs.heads:
    value: 4  # see GraphGPS https://github.com/rampasek/GraphGPS/blob/main/configs/GPS/zinc-GPS-LapPE%2BRWSE.yaml#L51
  imle_configs.rwse.kernel:
    value: 20
  imle_configs.rwse.layers:
    value: 2
  imle_configs.rwse.dim_pe:
    value: 16
  imle_configs.rwse.raw_norm_type:
    value: 'BatchNorm'
  imle_configs.lap.max_freqs:
    value: 4
  imle_configs.lap.dim_pe:
    value: 16
  imle_configs.lap.layers:
    value: 2
  imle_configs.lap.raw_norm_type:
    value: None
  imle_configs.emb_hid_size:
    values:
      - 32
      - 64
    distribution: categorical
  imle_configs.tf_layer:
    values:
      - 0
      - 2
      - 4
    distribution: categorical
  imle_configs.dropout:
    value: 0.
  imle_configs.attn_dropout:
    value: 0.5

  # upstream training
  imle_configs.embd_lr:
    min: 1.e-4
    max: 5.e-3
    distribution: uniform
  imle_configs.emb_optim:
    value: adamw
  imle_configs.emb_scheduler:
    value: cosine
  imle_configs.reg_embd:
    value: 0.
  imle_configs.layernorm:
    value: False
  imle_configs.batchnorm:
    value: True
  imle_configs.spectral_norm:
    value: True   # spectral normalization
  imle_configs.auxloss.variance:
    values: 
      - 0.
      - 0.1
      - 0.05
      - 0.01
    distribution: categorical
  imle_configs.auxloss.degree:
    value: 0.
  imle_configs.auxloss.origin_bias:
    values: 
      - 0.
      - 0.1
      - 0.05
      - 0.01
    distribution: categorical

  # downstream model
  model:
    value: gin_duo
  hid_size:
    value: 256
  num_convlayers:
    value: 4
  mlp_layers_intragraph:
    value: 3
  mlp_layers_intergraph:
    value: 2
  graph_pooling:
    value: mean
  dropout:
    value: 0.
  bn:
    value: True
  residual:
    value: True
  inter_graph_pooling:
    value: cat

  # downstream training
  rwse.kernel:
    value: 20
  rwse.layers:
    value: 2
  rwse.dim_pe:
    value: 16
  rwse.raw_norm_type:
    value: 'BatchNorm'
  lap.max_freqs:
    value: 4
  lap.dim_pe:
    value: 16
  lap.layers:
    value: 2
  lap.raw_norm_type:
    value: None

  lr:
    value: 1.e-3
  lr_decay.scheduler:
    value: step
  lr_decay.steps:
    value: '[ 400, 600 ]'
  optim:
    value: adam
  early_stop.patience:
    value: 100
  early_stop.target:
    value: val_metric
  reg:
    value: 0.
  batch_size:
    value: 128
  min_epochs:
    value: 700
  max_epochs:
    value: 1000
