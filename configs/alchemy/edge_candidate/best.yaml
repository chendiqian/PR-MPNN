fixed:
  dataset: alchemy
  log_path: './logs'

  use_wandb: True
  wandb_name: "best_alchemy"

  sample_configs:
    sample_policy: edge_candid
    separate: False
    directed: False
    in_place: True
    sample_k: 10   # edge addition
    sample_k2: 40   # edge deletion
    candid_pool: 200
    ensemble: 1
    heuristic: longest_path
    include_original_graph: True

  imle_configs:
    sampler: simple
    logits_activation: null
    weight_edges: marginals
    marginals_mask: True
    num_train_ensemble: 1
    num_val_ensemble: 5

    model: "edge_selector"
    rwse:
      kernel: 20
      layers: 2
      dim_pe: 32
      raw_norm_type: 'BatchNorm'

    lap:
      max_freqs: 4
      dim_pe: 32
      layers: 2
      raw_norm_type: null

    emb_hid_size: 128
    emb_optim: "adamw"
    emb_scheduler: None
    gnn_layer: 8
    mlp_layer: 4
    dropout: 0.
    embd_lr: 1.e-3
    reg_embd: 0.
    batchnorm: True

  rwse:
    kernel: 20
    layers: 2
    dim_pe: 16
    raw_norm_type: 'BatchNorm'
  lap:
    max_freqs: 4
    dim_pe: 16
    layers: 2
    raw_norm_type: null

  optim: "adam"
  model: gine_duo
  hid_size: 256
  lr: 1.e-3
  lr_decay:
    scheduler: 'plateau'
    decay_rate: 0.5
    mode: 'min'
    patience: 50
    target: 'val_metric'
  early_stop:
    patience: 200
    target: val_metric
  reg: 0.
  num_convlayers: 4
  mlp_layers_intragraph: 3
  mlp_layers_intergraph: 2
  graph_pooling: mean
  inter_graph_pooling: cat
  dropout: 0.
  bn: True
  residual: True
  batch_size: 128
  min_epochs: 400
  max_epochs: 1000
  data_path: './datasets'
  debug: False
  num_runs: 3